{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Neural Networks to NBA Team Win Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-2016 Misc\n",
      "2015-2016 Team\n",
      "2016-2017 Misc\n",
      "2016-2017 Team\n",
      "2017-2018 Misc\n",
      "2017-2018 Team\n",
      "2018-2019 Misc\n",
      "2018-2019 Team\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold all dataset names\n",
    "dataset_list = []\n",
    "\n",
    "# adds all CSV files from the datasets folder to a list\n",
    "for dataset in os.listdir('./win_predictor_data/'):\n",
    "    if (dataset.endswith('.csv')):\n",
    "        dataset_list.append(dataset)\n",
    "\n",
    "# adds all datasets to a dictionary with the key being 'Year Type' \n",
    "datasets = {}\n",
    "for dataset in dataset_list:\n",
    "    data_file = pd.read_csv('./win_predictor_data/' + dataset)\n",
    "    dataset_name = dataset[dataset.index('- ') + 1 : dataset.index('.')].strip()\n",
    "    print(dataset_name)\n",
    "    datasets[dataset_name] = data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove asterisks from team names\n",
    "def remove_asterisk(team):\n",
    "    if team.endswith(\"*\"):\n",
    "        return team[0:len(team)-1]\n",
    "    return team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_training_data(datasets):\n",
    "    \"\"\"\n",
    "    Creates the training data using the basketball datasets\n",
    "    @param datasets: A dictionary with year name & stat types as key and value of a pandas dataframe \n",
    "                         containing all the data \n",
    "    @return the training data and training data labels \n",
    "    \"\"\"\n",
    "    # create dictionary to hold aggregated datasets by year containing all (agg_datasets['2015-2016'])\n",
    "    agg_datasets = {}\n",
    "\n",
    "    # All the unwanted columns that won't be used in our neural network \n",
    "    unwanted_data = ['Rk_x', 'Team', 'Age', 'PW', 'PL', 'MOV', 'SOS', 'SRS', 'L', 'Arena', 'Attend.', 'Attend./G', 'Rk_y']\n",
    "\n",
    "    # run through each dataset and add to dictionary\n",
    "    for dataset in sorted(datasets):\n",
    "\n",
    "        # separate year\n",
    "        year_label = dataset[0 : dataset.index(' ')]\n",
    "\n",
    "        # remove asterisk from given dataset\n",
    "        datasets[dataset]['Team'] = datasets[dataset]['Team'].apply(remove_asterisk)\n",
    "\n",
    "        # add dataset to agg_datasets dictionary\n",
    "        if year_label in agg_datasets:\n",
    "            agg_datasets[year_label] = pd.merge(agg_datasets[year_label], datasets[dataset], on='Team')\n",
    "        else:\n",
    "            agg_datasets[year_label] = datasets[dataset]\n",
    "\n",
    "    # Aggregate all the data into one data frame and drop the unwanted features \n",
    "    agg_all_sets = pd.concat([agg_datasets[year_label] for year_label in agg_datasets], ignore_index = True)\n",
    "    agg_all_sets = agg_all_sets.drop(unwanted_data, axis = 1)\n",
    "\n",
    "\n",
    "    # Convert the aggregated data into a training set \n",
    "    training_data = [agg_all_sets.loc[i,:] for i in range(len(agg_all_sets))]\n",
    "    training_data = [team[1:] for team in training_data]\n",
    "\n",
    "    # Get the training data labels (Wins) from the training_data \n",
    "    training_data_labels = [team[0] for team in training_data]\n",
    "\n",
    "    return torch.tensor(training_data), torch.tensor(training_data_labels) \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_examples, num_features):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_size =  math.floor((2/3)*num_features)\n",
    "        self.l1 = nn.Linear(num_examples, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[114.5000, 103.8000,  10.7000,  ...,  15.2000,  20.7000, 114.9000],\n",
       "        [110.3000,  99.0000,  11.3000,  ...,  13.1000,  17.5000, 103.5000],\n",
       "        [113.1000, 105.6000,   7.5000,  ...,  15.9000,  20.6000, 110.2000],\n",
       "        ...,\n",
       "        [105.9000, 115.1000,  -9.2000,  ...,  15.6000,  23.6000, 107.5000],\n",
       "        [104.5000, 113.7000,  -9.2000,  ...,  14.0000,  20.9000, 104.6000],\n",
       "        [107.7000, 117.6000,  -9.9000,  ...,  13.5000,  20.0000, 104.5000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the training data into tensors \n",
    "training_data, training_data_labels = create_training_data(datasets)\n",
    "print(len(training_data[0]), len(training_data_labels))\n",
    "\n",
    "net = Net(len(training_data), len(training_data[0]))\n",
    "\n",
    "# out = net(training_data)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDos\n",
    "- Convert Data into Proper Format of Neural Net\n",
    "- Research Neural Network architectures for predictions (watch tutorials/videos)\n",
    "- set up PyTorch/Keras Framework to create neural network\n",
    "- EDA (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
